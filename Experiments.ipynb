{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workers import MasterNode\n",
    "from models import LinReg, LogReg, LogRegNoncvx, NN_1d_regression\n",
    "from utils import read_run, get_alg, create_plot_dir, PLOT_PATH\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prep_data import number_of_features\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from numpy.random import default_rng\n",
    "from numpy import linalg as la\n",
    "from prep_data import DATASET_PATH\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fast')\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "# mpl.rcParams['mathtext.fontset'] = 'dejavusans'\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['lines.linewidth'] = 2.0\n",
    "mpl.rcParams['legend.fontsize'] = 'large'\n",
    "mpl.rcParams['axes.titlesize'] = 'xx-large'\n",
    "mpl.rcParams['xtick.labelsize'] = 'x-large'\n",
    "mpl.rcParams['ytick.labelsize'] = 'x-large'\n",
    "mpl.rcParams['axes.labelsize'] = 'xx-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ['x', '.', '+', '1', 'p','*', 'D' , '.',  's']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DIANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.1, 1.0, 10)\n",
    "dataset_name = 'mushrooms'\n",
    "n_workers = 8\n",
    "exp = 'diana'\n",
    "max_it = 1000\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True\n",
    "\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_diana_sparsification(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 800\n",
    "\n",
    "alphas_shown = alphas[::2]\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ijcnn1.bz2'\n",
    "alg = LogReg\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_diana_sparsification(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 250\n",
    "\n",
    "alphas_shown = alphas[::2]\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'w6a'\n",
    "alg = LogReg\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1\n",
    "max_it = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_diana_sparsification(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 2400\n",
    "\n",
    "alphas_shown = alphas[::2]\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'a6a'\n",
    "alg = LogReg\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1\n",
    "max_it = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_diana_sparsification(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 1000\n",
    "\n",
    "alphas_shown = alphas[::2]\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([1.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "dataset_name = 'mushrooms'\n",
    "n_workers = 8\n",
    "exp = 'gd'\n",
    "max_it = 200\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('------------------- alpha = {} --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    print('Running GD...')\n",
    "    model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 200\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, [alpha] * n_workers, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery +  2 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 2 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {}'.format(np.format_float_scientific(alpha, trim='-', exp_digits=1)) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Squared distance')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ijcnn1.bz2'\n",
    "n_workers = 8\n",
    "exp = 'gd'\n",
    "max_it = 100\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('-------------------- alpha = {} --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 25\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, [alpha] * n_workers, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(2, 2), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(2, 2), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {}'.format(np.format_float_scientific(alpha, trim='-', exp_digits=1)) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Squared distance')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'w6a'\n",
    "n_workers = 8\n",
    "exp = 'gd'\n",
    "max_it = 200\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('-------------------- alpha = {} --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    print('Running GD...')\n",
    "    model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 75\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, [alpha] * n_workers, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {}'.format(np.format_float_scientific(alpha, trim='-', exp_digits=1)) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Squared distance')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([1.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "dataset_name = 'a6a'\n",
    "n_workers = 8\n",
    "exp = 'gd'\n",
    "max_it = 200\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 175\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, [alpha] * n_workers, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(10 + 3 * ind, 13), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(10 + 3 * ind, 13), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {}'.format(np.format_float_scientific(alpha, trim='-', exp_digits=1)) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Squared distance')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compressed Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mushrooms'\n",
    "exp= 'cgd'\n",
    "alg = LogReg\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1\n",
    "max_it = 1000\n",
    "alphas = np.linspace(0.1, 1.0, 10)[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_cgd(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 1000\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ijcnn1.bz2'\n",
    "exp= 'cgd'\n",
    "alg = LogReg\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1\n",
    "max_it = 1000\n",
    "alphas = np.linspace(0.1, 1.0, 10)[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_cgd(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 200\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'w6a'\n",
    "exp= 'cgd'\n",
    "alg = LogReg\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1\n",
    "max_it = 1000\n",
    "alphas = np.linspace(0.1, 1.0, 10)[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_cgd(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 200\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'a6a'\n",
    "exp= 'cgd'\n",
    "alg = LogReg\n",
    "d = number_of_features(dataset_name)\n",
    "ks = np.random.randint(d, size=n_workers)\n",
    "ks += 1\n",
    "max_it = 1000\n",
    "alphas = np.linspace(0.1, 1.0, 10)[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    print('alpha = {} \\n --------------------'.format(alpha))\n",
    "    model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "    model.run_cgd(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(7, 10), constrained_layout=True)\n",
    "n_iter_shown = 1000\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind = 0\n",
    "\n",
    "for alpha in alphas_shown:\n",
    "    run = read_run(exp, alpha, dataset_name, logreg)\n",
    "\n",
    "    f_values = run['fval'][:n_iter_shown]\n",
    "    dists = run['dist'][:n_iter_shown]\n",
    "    \n",
    "    markevery = int(f_values.size / 10)\n",
    "\n",
    "    axs[0].plot(dists, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    axs[1].plot(f_values, marker=markers[ind], markevery=(markevery + 20 * ind, markevery), markersize=10)\n",
    "    ind += 1\n",
    "\n",
    "axs[0].legend([r'$\\alpha$ = {:.1f}'.format(alpha) for alpha in alphas_shown])\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_xlabel('Communication rounds')\n",
    "axs[0].set_ylabel('Expected squared distance')\n",
    "axs[1].set_ylabel('Expected loss')\n",
    "axs[0].set_title(dataset_name)\n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_' + dataset_name\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CGD experiments with fixed sparsification parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['mushrooms', 'ijcnn1.bz2', 'w6a', 'a6a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([1.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "n_workers = 8\n",
    "exp = 'cgd'\n",
    "max_it = 300\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    for alpha in alphas:\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "        for k in k_array:\n",
    "            ks = np.array([k] * n_workers, dtype=np.int)\n",
    "            print('alpha = {} k = {} \\n --------------------'.format(alpha, k))\n",
    "            print('Running Compressed Gradient Descent...')\n",
    "            model.run_cgd(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=len(datasets), ncols=len(alphas), sharey=True, figsize=(5 * len(alphas) , 5 * len(datasets)), constrained_layout=True)\n",
    "\n",
    "n_iter_shown = 300\n",
    "\n",
    "alphas_shown = alphas\n",
    "ind_ext1 = 0\n",
    "ind_ext2 = 0\n",
    "ind_int = 0\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    ind_ext1 = 0\n",
    "    for alpha in alphas_shown:\n",
    "        ind_int = 0\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        \n",
    "        k_array_shown = k_array\n",
    "        \n",
    "        for k in k_array_shown:    \n",
    "            run = read_run(exp, [alpha] * n_workers, dataset_name, logreg, [k] * n_workers)\n",
    "            f_values = run['fval'][:n_iter_shown]\n",
    "            \n",
    "            f_values = f_values[f_values > 1e-15]\n",
    "\n",
    "            markevery = int(f_values.size / 10)\n",
    "            axs[ind_ext2, ind_ext1].plot(f_values, marker=markers[ind_int], markevery=(markevery + 2 * ind_int, markevery), markersize=10)\n",
    "            ind_int += 1\n",
    "\n",
    "\n",
    "\n",
    "        axs[ind_ext2, ind_ext1].legend([r'$k$ = {}'.format(k) for k in k_array_shown])\n",
    "        axs[ind_ext2, ind_ext1].set_yscale('log')\n",
    "        axs[ind_ext2, ind_ext1].set_xlabel('Communication rounds')\n",
    "        axs[ind_ext2, ind_ext1].set_ylabel('Loss')\n",
    "        axs[ind_ext2, ind_ext1].set_title(dataset_name + r', $\\alpha$ = {}'.format(alpha) )\n",
    "        axs[ind_ext2, ind_ext1].yaxis.set_tick_params(labelbottom=True)\n",
    "        ind_ext1 += 1\n",
    "    ind_ext2 += 1\n",
    "    \n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_all'\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing CGD with GD in terms of communication cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comm_cost = 50000\n",
    "alg = LogReg\n",
    "for dataset_name in datasets:\n",
    "    for alpha in alphas:\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, 1500)\n",
    "        for k in k_array:\n",
    "            max_it = int(math.ceil(max_comm_cost / (n_workers * k)))\n",
    "            ks = np.array([k] * n_workers, dtype=np.int)\n",
    "            print('---------------alpha = {}, k = {}, datasets = {} --------------------'.format(alpha, k, dataset_name))\n",
    "            print('Running Compressed Gradient Descent...')\n",
    "            model.run_cgd(ks, max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_shown = alphas[:-1]\n",
    "comm_cost_shown = 50000\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(datasets), ncols=len(alphas_shown), sharey='row', figsize=(5 * len(alphas) , 5 * len(datasets)), constrained_layout=True)\n",
    "\n",
    "ind_ext1 = 0\n",
    "ind_ext2 = 0\n",
    "ind_int = 0\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    ind_ext1 = 0\n",
    "    for alpha in alphas_shown:\n",
    "        ind_int = 0\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        \n",
    "        k_array_shown = k_array\n",
    "        \n",
    "        for k in k_array_shown:    \n",
    "            run = read_run(exp, [alpha] * n_workers, dataset_name, logreg, [k] * n_workers)\n",
    "            f_values = run['fval']\n",
    "            f_values = f_values[f_values > 1e-9]\n",
    "            comm_cost = np.array([i * k * n_workers for i in range(len(f_values))])\n",
    "            \n",
    "            \n",
    "            f_values = f_values[comm_cost < comm_cost_shown]\n",
    "            comm_cost = comm_cost[comm_cost < comm_cost_shown]\n",
    "            \n",
    "            markevery = int(math.sqrt(len(f_values)))\n",
    "\n",
    "            axs[ind_ext2, ind_ext1].plot(comm_cost / 1000, f_values, marker=markers[ind_int], markevery=(2, markevery), markersize=10)\n",
    "            ind_int += 1\n",
    "\n",
    "\n",
    "\n",
    "        axs[ind_ext2, ind_ext1].legend([r'$k$ = {}'.format(k) for k in k_array_shown])\n",
    "        axs[ind_ext2, ind_ext1].set_yscale('log')\n",
    "        axs[ind_ext2, ind_ext1].set_xlabel('Communication cost')\n",
    "        axs[ind_ext2, ind_ext1].set_ylabel('Loss')\n",
    "        # axs[ind_ext2, ind_ext1].set_ylim(bottom=1e-8)\n",
    "        axs[ind_ext2, ind_ext1].set_title(dataset_name + r', $\\alpha$ = {}'.format(alpha) )\n",
    "        axs[ind_ext2, ind_ext1].yaxis.set_tick_params(labelbottom=True)\n",
    "        ind_ext1 += 1\n",
    "    ind_ext2 += 1\n",
    "    \n",
    "alg = get_alg(logreg)\n",
    "name = exp + '_' + alg + '_all_comm_cost'\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing DIANA to GD in terms of communication rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['mushrooms', 'ijcnn1.bz2', 'w6a', 'a6a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([1.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "n_workers = 50\n",
    "exp = 'diana'\n",
    "max_it = 2000\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    print('------------- {} -------------'.format(dataset_name))\n",
    "    for alpha in alphas:\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        print('----------- alpha = {} ------------'.format(alpha))\n",
    "        model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "        for k in k_array:\n",
    "            ks = np.array([k] * n_workers, dtype=np.int)\n",
    "            print('----------------- k = {} --------------------'.format(k))\n",
    "            print('Running DIANA...')\n",
    "            model.run_diana_sparsification(ks, max_it)\n",
    "        print('Running Gradient Descent...')\n",
    "        model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_shown = alphas[:-1]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(datasets), ncols=len(alphas_shown), sharey=True, figsize=(5 * len(alphas_shown) , 5 * len(datasets)), constrained_layout=True)\n",
    "\n",
    "n_iter_shown = 2000\n",
    "\n",
    "ind_ext1 = 0\n",
    "ind_ext2 = 0\n",
    "ind_int = 0\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    ind_ext1 = 0\n",
    "    for alpha in alphas_shown:\n",
    "        ind_int = 0\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        \n",
    "        k_array_shown = k_array\n",
    "        \n",
    "        for k in k_array_shown:    \n",
    "            run = read_run('diana', [alpha] * n_workers, dataset_name, logreg, [k] * n_workers)\n",
    "            f_values = run['fval'][:n_iter_shown]\n",
    "            \n",
    "            f_values = f_values[f_values > 1e-15]\n",
    "\n",
    "            markevery = int(f_values.size / 10)\n",
    "            axs[ind_ext2, ind_ext1].plot(f_values, marker=markers[ind_int], markevery=(markevery + 2 * ind_int, markevery), markersize=10, \n",
    "                                        label = r'$k$ = {}'.format(k))\n",
    "            ind_int += 1\n",
    "            \n",
    "        run = read_run('gd', [alpha] * n_workers, dataset_name, logreg)\n",
    "        f_values = run['fval'][:n_iter_shown]\n",
    "        f_values = f_values[f_values > 1e-15]\n",
    "        markevery = int(f_values.size / 10)\n",
    "        axs[ind_ext2, ind_ext1].plot(f_values, marker=markers[ind_int], markevery=(markevery + 2 * ind_int, markevery), markersize=10, label='GD')\n",
    "\n",
    "\n",
    "        axs[ind_ext2, ind_ext1].set_yscale('log')\n",
    "        axs[ind_ext2, ind_ext1].set_xlabel('Communication rounds')\n",
    "        axs[ind_ext2, ind_ext1].set_ylabel('Loss')\n",
    "        axs[ind_ext2, ind_ext1].set_title(dataset_name + r', $\\alpha$ = {}'.format(alpha) )\n",
    "        axs[ind_ext2, ind_ext1].yaxis.set_tick_params(labelbottom=True)\n",
    "        axs[ind_ext2, ind_ext1].legend()\n",
    "        ind_ext1 += 1\n",
    "    ind_ext2 += 1\n",
    "    \n",
    "alg = get_alg(logreg)\n",
    "name = 'diana' + '_' + alg + '_all(n=50)'\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing DIANA to GD in terms of communication cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['mushrooms', 'ijcnn1.bz2', 'w6a', 'a6a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.array([1.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "n_workers = 50\n",
    "exp = 'diana'\n",
    "max_comm_cost = 50000\n",
    "\n",
    "alg = LogReg\n",
    "logreg = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    print('------------------------------- {} -------------------------------'.format(dataset_name))\n",
    "    for alpha in alphas:\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        print('---------------------------- alpha = {} ----------------------------'.format(alpha))\n",
    "        model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, 1500)\n",
    "        for k in k_array:\n",
    "            ks = np.array([k] * n_workers, dtype=np.int)\n",
    "            print('------------------------- k = {} -------------------------'.format(k))\n",
    "            print('Running DIANA...')\n",
    "            max_it = int(math.ceil(max_comm_cost / (n_workers * k)))\n",
    "            model.run_diana_sparsification(ks, max_it)\n",
    "        print('Running Gradient Descent...')\n",
    "        max_it = int(math.ceil(max_comm_cost / (n_workers * d)))\n",
    "        model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_shown = alphas[:-1]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(datasets), ncols=len(alphas_shown), sharey=True, figsize=(5 * len(alphas_shown) , 5 * len(datasets)), constrained_layout=True)\n",
    "\n",
    "# n_iter_shown = 2000\n",
    "\n",
    "ind_ext1 = 0\n",
    "ind_ext2 = 0\n",
    "ind_int = 0\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    ind_ext1 = 0\n",
    "    for alpha in alphas_shown:\n",
    "        ind_int = 0\n",
    "        d = number_of_features(dataset_name)\n",
    "        k_array = np.linspace(0.2, 1.0, 5) * d\n",
    "        k_array = np.array(k_array, dtype=np.int)\n",
    "        k_array = np.insert(k_array, 0, 1)\n",
    "        \n",
    "        k_array_shown = k_array\n",
    "        \n",
    "        for k in k_array_shown:    \n",
    "            run = read_run('diana', [alpha] * n_workers, dataset_name, logreg, [k] * n_workers)\n",
    "            f_values = run['fval']\n",
    "            comm_cost = np.array([i * k * n_workers for i in range(len(f_values))])\n",
    "            \n",
    "            comm_cost = comm_cost[f_values > 1e-15]\n",
    "            f_values = f_values[f_values > 1e-15]\n",
    "\n",
    "            markevery = max(int(f_values.size / 10), 2)\n",
    "            axs[ind_ext2, ind_ext1].plot(comm_cost / 1000, f_values, marker=markers[ind_int], markevery=(markevery + 2 * ind_int, markevery), markersize=10, \n",
    "                                        label = r'$k$ = {}'.format(k))\n",
    "            ind_int += 1\n",
    "    \n",
    "            \n",
    "        run = read_run('gd', [alpha] * n_workers, dataset_name, logreg)\n",
    "        f_values = run['fval']\n",
    "        comm_cost = np.array([i * d * n_workers for i in range(len(f_values))])\n",
    "\n",
    "            \n",
    "        comm_cost = comm_cost[f_values > 1e-15]\n",
    "        f_values = f_values[f_values > 1e-15]\n",
    "        \n",
    "        markevery = max(int(f_values.size / 10), 2)\n",
    "        axs[ind_ext2, ind_ext1].plot(comm_cost / 1000, f_values, marker=markers[ind_int], markevery=(markevery + 2 * ind_int, markevery), markersize=10, label='GD')\n",
    "\n",
    "\n",
    "        axs[ind_ext2, ind_ext1].set_yscale('log')\n",
    "        axs[ind_ext2, ind_ext1].set_xlabel('Communication cost')\n",
    "        axs[ind_ext2, ind_ext1].set_ylabel('Loss')\n",
    "        axs[ind_ext2, ind_ext1].set_title(dataset_name + r', $\\alpha$ = {}'.format(alpha) )\n",
    "        axs[ind_ext2, ind_ext1].yaxis.set_tick_params(labelbottom=True)\n",
    "        axs[ind_ext2, ind_ext1].legend()\n",
    "        ind_ext1 += 1\n",
    "    ind_ext2 += 1\n",
    "    \n",
    "alg = get_alg(logreg)\n",
    "name = 'diana' + '_' + alg + '_all_comm_cost(n=50)'\n",
    "create_plot_dir()\n",
    "plt.savefig(PLOT_PATH + '/' + name + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sine Meta Learning experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tasks = 20\n",
    "number_of_points_per_task = 50\n",
    "dataset_name = 'artificial_sine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sine = np.empty(shape=(number_of_tasks * number_of_points_per_task, 1))\n",
    "y_sine = np.empty(shape=(number_of_tasks * number_of_points_per_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng()\n",
    "a_array = []\n",
    "b_array = []\n",
    "for i in range(number_of_tasks):\n",
    "    a = 0.1 + rng.random() * (5.0 - 0.1)\n",
    "    a_array.append(a)\n",
    "    b = rng.random() * 2 * np.pi\n",
    "    b_array.append(b)\n",
    "    print('a = {}, b = {}'.format(a, b))\n",
    "    x_train = -5.0 + rng.random((number_of_points_per_task, 1)) * 10.0\n",
    "    y_train = a * np.sin(x_train + b)\n",
    "    X_sine[number_of_points_per_task * i : number_of_points_per_task * (i + 1)] = x_train.copy()\n",
    "    y_sine[number_of_points_per_task * i : number_of_points_per_task * (i + 1)] = y_train.squeeze(1).copy()\n",
    "a_array = np.array(a_array)\n",
    "b_array = np.array(b_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(5, number_of_tasks)):\n",
    "    inds = range(number_of_points_per_task * i, number_of_points_per_task * (i + 1))\n",
    "    plt.scatter(X_sine[inds], y_sine[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X_sine, y_sine, DATASET_PATH + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATASET_PATH + 'a.npy', a_array)\n",
    "np.save(DATASET_PATH + 'b.npy', b_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "n_workers = number_of_tasks\n",
    "exp = 'gd'\n",
    "max_it = 100\n",
    "\n",
    "alg = NN_1d_regression\n",
    "logreg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- alpha = {} --------------------'.format(alpha))\n",
    "model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "print('Running GD...')\n",
    "model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rerun GD for some workers in order to achieve required precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_id = 6\n",
    "worker_to_be_fit = model.workers[worker_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(worker_to_be_fit.fun_value(worker_to_be_fit.w_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.1\n",
    "max_L = 0.1\n",
    "max_it = 100000\n",
    "tol = 1e-2\n",
    "w = np.random.randn(worker_to_be_fit.iterate_size())\n",
    "max_L_constant = 0.1 * 2 ** 40\n",
    "grad_norm = None\n",
    "min_f_value = float('Inf')\n",
    "\n",
    "for it in range(max_it):\n",
    "    grad = worker_to_be_fit.grad(worker_to_be_fit.x_train, worker_to_be_fit.y_train, w)\n",
    "    L = 0.1\n",
    "    curr_fun_value = worker_to_be_fit.fun_value(w)\n",
    "    \n",
    "    while True:\n",
    "        if L > max_L_constant: # if L becomes too large, jump to another random point w\n",
    "            w = np.random.randn(worker_to_be_fit.iterate_size())\n",
    "            grad = worker_to_be_fit.grad(worker_to_be_fit.x_train, worker_to_be_fit.y_train, w)\n",
    "            L = 0.1\n",
    "            curr_fun_value = worker_to_be_fit.fun_value(w)\n",
    "\n",
    "        print('Current L = {:f}'.format(L), end='\\r')\n",
    "            \n",
    "        f_value_ = worker_to_be_fit.fun_value(w - grad / L)\n",
    "        if curr_fun_value - f_value_ > 0:\n",
    "            break\n",
    "        L *= 2.0\n",
    "        \n",
    "    w -= grad / L\n",
    "    grad_norm = la.norm(grad)\n",
    "    \n",
    "    if f_value_ < min_f_value:\n",
    "        min_f_value = f_value_\n",
    "        worker_to_be_fit.memory = w\n",
    "    \n",
    "    if max_L < L:\n",
    "        max_L = L\n",
    "    print('                               {:5d}/{:5d} Iterations: fun_value {:f} grad_norm {:f}'.format(it+1, max_it, f_value_, grad_norm), end='\\r')                \n",
    "    if grad_norm < tol and f_value_ < tol ** 2:\n",
    "          break\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_to_be_fit.w_opt = copy.deepcopy(worker_to_be_fit.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check loss values for pure local models estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker_id in range(model.n_workers):\n",
    "    print(model.workers[worker_id].fun_value(model.workers[worker_id].w_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values are below $10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker_id in range(model.n_workers):\n",
    "    print(model.workers[worker_id].alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_L = 1e-6\n",
    "max_L = 0.1\n",
    "max_it = 50000\n",
    "tol = 1e-2\n",
    "max_L_constant = 2 ** 40\n",
    "grad_norm = None\n",
    "min_f_value = float('Inf')\n",
    "memory = None\n",
    "\n",
    "try:\n",
    "    for it in range(max_it):\n",
    "        grad = model.grad(w)\n",
    "        L = min_L\n",
    "        curr_fun_value = model.fun_value(w)\n",
    "\n",
    "        while True:\n",
    "            if L > max_L_constant: # if L becomes too large, jump to another random point w\n",
    "                w = np.random.randn(self.iterate_size())\n",
    "                grad = model.grad(w)\n",
    "                L = min_L\n",
    "                curr_fun_value = model.fun_value(w)\n",
    "\n",
    "            print('Current L = {:f}'.format(L), end='\\r')\n",
    "\n",
    "            f_value_ = model.fun_value(w - grad / L)\n",
    "            if curr_fun_value - f_value_ > 0:\n",
    "                break\n",
    "            L *= 2.0\n",
    "\n",
    "        w -= grad / L\n",
    "        grad_norm = la.norm(grad)\n",
    "\n",
    "        if f_value_ < min_f_value:\n",
    "            min_f_value = f_value_\n",
    "            memory = w\n",
    "\n",
    "        if max_L < L:\n",
    "            max_L = L\n",
    "        print('                               {:5d}/{:5d} Iterations: fun_value {:f} grad_norm {:f}'.format(it+1, max_it, f_value_, grad_norm), end='\\r')                \n",
    "        if grad_norm < tol and f_value_ < tol ** 2:\n",
    "              break\n",
    "except KeyboardInterrupt:\n",
    "    print('')\n",
    "    print(min_f_value)\n",
    "else:\n",
    "    print('')\n",
    "    print('Unknown problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w_opt_global = copy.deepcopy(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w_opt_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global model performance on a worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 1000\n",
    "mse_local_arr = []\n",
    "mse_global_arr = []\n",
    "for worker_id in range(model.n_workers):\n",
    "    x_test_int = -5.0 + rng.random((n_test, 1)) * 10.0\n",
    "    y_test_int = a_array[worker_id] * np.sin(x_test_int + b_array[worker_id])\n",
    "    model.workers[worker_id].set_weights(model.workers[worker_id].w_opt)\n",
    "    y_test_local = model.workers[worker_id].model(torch.from_numpy(x_test_int).float()).detach().numpy()\n",
    "    model.workers[worker_id].set_weights(model.workers[worker_id].compute_local(model.w_opt_global))\n",
    "    y_test_global = model.workers[worker_id].model(torch.from_numpy(x_test_int).float()).detach().numpy()\n",
    "    mse_local = np.mean((y_test_int - y_test_local) ** 2)\n",
    "    mse_global = np.mean((y_test_int - y_test_global) ** 2)\n",
    "    mse_local_arr.append(mse_local)\n",
    "    mse_global_arr.append(mse_global)\n",
    "    print('Worker {}: mse for local model = {}, mse for generalized model = {}'.format(worker_id, mse_local, mse_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_local_arr, label='local')\n",
    "plt.plot(mse_global_arr, label='global')\n",
    "plt.xlabel('Worker id')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "# plt.savefig('tmp.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mse_local_arr, mse_global_arr)\n",
    "lim_ = max(max(mse_global_arr), max(mse_local_arr))\n",
    "line = np.linspace(0, 0.5, 20)\n",
    "plt.xlabel('MSE for local models')\n",
    "plt.ylabel('MSE for generalizes models')\n",
    "plt.xlim([0, lim_])\n",
    "plt.ylim([0, lim_])\n",
    "plt.plot(line, line)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tmp2.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_initial = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_init_dict = {'model' : model_initial}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_initial', 'wb') as file:\n",
    "    pickle.dump(model_init_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_initial', 'rb') as file:\n",
    "    model_initial = pickle.load(file)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(10):\n",
    "    models.append(copy.deepcopy(model_initial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0, 1.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    print(i)\n",
    "    if i != 5:\n",
    "        models[i].change_alpha(alphas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(models[i].workers[0].alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].w_opt_global = np.zeros(models[0].d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].w_opt_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    model = models[i]\n",
    "    min_L = 1e-6\n",
    "    max_L = 0.1\n",
    "    max_it = 100000\n",
    "    tol = 1e-2\n",
    "    max_L_constant = 2 ** 40\n",
    "    w = copy.deepcopy(models[i-1].w_opt_global)\n",
    "    grad_norm = None\n",
    "    min_f_value = float('Inf')\n",
    "\n",
    "    try:\n",
    "        for it in range(max_it):\n",
    "            grad = model.grad(w)\n",
    "            L = min_L\n",
    "            curr_fun_value = model.fun_value(w)\n",
    "\n",
    "            while True:\n",
    "                if L > max_L_constant: # if L becomes too large, jump to another random point w\n",
    "                    w = np.random.randn(model.d)\n",
    "                    grad = model.grad(w)\n",
    "                    L = min_L\n",
    "                    curr_fun_value = model.fun_value(w)\n",
    "\n",
    "                print('Current L = {:f}'.format(L), end='\\r')\n",
    "\n",
    "                f_value_ = model.fun_value(w - grad / L)\n",
    "                if curr_fun_value - f_value_ > 0:\n",
    "                    break\n",
    "                L *= 2.0\n",
    "\n",
    "            w -= grad / L\n",
    "            grad_norm = la.norm(grad)\n",
    "\n",
    "            if f_value_ < min_f_value:\n",
    "                min_f_value = f_value_\n",
    "                model.w_opt_global = copy.deepcopy(w)\n",
    "\n",
    "            if max_L < L:\n",
    "                max_L = L\n",
    "            print('                               {:5d}/{:5d} Iterations: fun_value {:f} grad_norm {:f}'.format(it+1, max_it, f_value_, grad_norm), end='\\r')                \n",
    "            if grad_norm < tol and f_value_ < tol:\n",
    "                  break\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print(min_f_value)\n",
    "    else:\n",
    "        print('')\n",
    "        print('Unknown problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].change_alpha(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].w_opt_global = np.zeros(models[0].d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(10):\n",
    "    print(models[ind].fun_value(models[ind].w_opt_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'models' : models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models', 'wb') as file:\n",
    "    pickle.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models', 'rb') as file:\n",
    "    models = pickle.load(file)['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_initial', 'rb') as file:\n",
    "    model_1 = pickle.load(file)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.change_alpha(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_1\n",
    "max_L = 0.1\n",
    "min_L = 1e-10\n",
    "max_it = 100000\n",
    "tol = 1e-2\n",
    "max_L_constant = 2 ** 40\n",
    "w = copy.deepcopy(models[-1].w_opt_global)\n",
    "grad_norm = None\n",
    "min_f_value = float('Inf')\n",
    "\n",
    "try:\n",
    "    for it in range(max_it):\n",
    "        grad = model.grad(w)\n",
    "        L = min_L\n",
    "        curr_fun_value = model.fun_value(w)\n",
    "\n",
    "        while True:\n",
    "            if L > max_L_constant: # if L becomes too large, jump to another random point w\n",
    "                w = np.random.randn(model.d)\n",
    "                grad = model.grad(w)\n",
    "                L = min_L\n",
    "                curr_fun_value = model.fun_value(w)\n",
    "\n",
    "            print('Current L = {:f}'.format(L), end='\\r')\n",
    "\n",
    "            f_value_ = model.fun_value(w - grad / L)\n",
    "            if curr_fun_value - f_value_ > 0:\n",
    "                break\n",
    "            L *= 2.0\n",
    "\n",
    "        w -= grad / L\n",
    "        grad_norm = la.norm(grad)\n",
    "\n",
    "        if f_value_ < min_f_value:\n",
    "            min_f_value = f_value_\n",
    "            model.w_opt_global = copy.deepcopy(w)\n",
    "\n",
    "        if max_L < L:\n",
    "            max_L = L\n",
    "        print('                               {:5d}/{:5d} Iterations: fun_value {:f} grad_norm {:f}'.format(it+1, max_it, f_value_, grad_norm), end='\\r')                \n",
    "        if grad_norm < tol and f_value_ < tol:\n",
    "              break\n",
    "except KeyboardInterrupt:\n",
    "    print('')\n",
    "    print(min_f_value)\n",
    "else:\n",
    "    print('')\n",
    "    print('Unknown problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in MSE over alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_table = np.empty(shape=(10, models[0].n_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(models[i].workers[0].alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_array = np.load(DATASET_PATH + 'a.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_array = np.load(DATASET_PATH + 'b.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 2000\n",
    "rng = default_rng()\n",
    "for i in range(models[0].n_workers):\n",
    "    x_test = -5.0 + rng.random((n_test, 1)) * 10\n",
    "    y_test = a_array[i] * np.sin(x_test + b_array[i])\n",
    "    for j in range(10):\n",
    "        worker = models[j].workers[i]\n",
    "        worker.set_weights(worker.compute_local(models[j].w_opt_global))\n",
    "        y_pred = worker.model(torch.from_numpy(x_test).float()).detach().numpy()\n",
    "        mse = np.mean((y_pred - y_test) ** 2).item()\n",
    "        mse_table[j][i] = copy.copy(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_alpha = np.mean(mse_table, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0.0, 1.0, 0.1), mse_alpha)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.xlim([0, 0.9])\n",
    "plt.ylim([0, max(mse_alpha)])\n",
    "plt.xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_PATH + '/sine_varied_alphas.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrowing task space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tasks = 200\n",
    "number_of_points_per_task = 50\n",
    "dataset_name = 'artificial_sine_extended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sine = np.empty(shape=(number_of_tasks * number_of_points_per_task, 1))\n",
    "y_sine = np.empty(shape=(number_of_tasks * number_of_points_per_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng()\n",
    "a_array = []\n",
    "b_array = []\n",
    "for i in range(number_of_tasks):\n",
    "    a = 0.1 + rng.random() * (2.0 - 0.1)\n",
    "    a_array.append(a)\n",
    "    b = rng.random() * 2 * np.pi\n",
    "    b_array.append(b)\n",
    "    x_train = -5.0 + rng.random((number_of_points_per_task, 1)) * 10.0\n",
    "    y_train = a * np.sin(x_train + b)\n",
    "    X_sine[number_of_points_per_task * i : number_of_points_per_task * (i + 1)] = x_train.copy()\n",
    "    y_sine[number_of_points_per_task * i : number_of_points_per_task * (i + 1)] = y_train.squeeze(1).copy()\n",
    "a_array = np.array(a_array)\n",
    "b_array = np.array(b_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(5, number_of_tasks)):\n",
    "    inds = range(number_of_points_per_task * i, number_of_points_per_task * (i + 1))\n",
    "    plt.scatter(X_sine[inds], y_sine[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X_sine, y_sine, DATASET_PATH + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATASET_PATH + 'a_ext.npy', a_array)\n",
    "np.save(DATASET_PATH + 'b_ext.npy', b_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_array = np.load(DATASET_PATH + 'a_ext.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_array = np.load(DATASET_PATH + 'b_ext.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training each local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "n_workers = number_of_tasks\n",
    "exp = 'gd'\n",
    "max_it = 100\n",
    "\n",
    "alg = NN_1d_regression\n",
    "logreg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- alpha = {} --------------------'.format(alpha))\n",
    "model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it)\n",
    "print('Running GD...')\n",
    "model.run_gd(max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in_dict = {'model' : model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_initial_extended', 'wb') as file:\n",
    "    pickle.dump(model_in_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(10):\n",
    "    models.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0, 1.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    models[i].change_alpha(alphas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].w_opt_global = np.zeros(models[0].d)\n",
    "models[0].alpha = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 10):\n",
    "    model = models[i]\n",
    "    min_L = 1e-6\n",
    "    max_L = 0.1\n",
    "    max_it = 100000\n",
    "    tol = 1e-2\n",
    "    max_L_constant = 2 ** 40\n",
    "    w = copy.deepcopy(models[i-1].w_opt_global)\n",
    "    grad_norm = None\n",
    "    min_f_value = float('Inf')\n",
    "\n",
    "    try:\n",
    "        for it in range(max_it):\n",
    "            grad = model.grad(w)\n",
    "            L = min_L\n",
    "            curr_fun_value = model.fun_value(w)\n",
    "\n",
    "            while True:\n",
    "                if L > max_L_constant: # if L becomes too large, jump to another random point w\n",
    "                    w = np.random.randn(model.d)\n",
    "                    grad = model.grad(w)\n",
    "                    L = min_L\n",
    "                    curr_fun_value = model.fun_value(w)\n",
    "\n",
    "                print('Current L = {:f}'.format(L), end='\\r')\n",
    "\n",
    "                f_value_ = model.fun_value(w - grad / L)\n",
    "                if curr_fun_value - f_value_ > 0:\n",
    "                    break\n",
    "                L *= 2.0\n",
    "\n",
    "            w -= grad / L\n",
    "            grad_norm = la.norm(grad)\n",
    "\n",
    "            if f_value_ < min_f_value:\n",
    "                min_f_value = f_value_\n",
    "                model.w_opt_global = copy.deepcopy(w)\n",
    "\n",
    "            if max_L < L:\n",
    "                max_L = L\n",
    "            print('                               {:5d}/{:5d} Iterations: fun_value {:f} grad_norm {:f}'.format(it+1, max_it, f_value_, grad_norm), end='\\r')                \n",
    "            if grad_norm < tol and f_value_ < tol:\n",
    "                  break\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print(min_f_value)\n",
    "    else:\n",
    "        print('')\n",
    "        print('Unknown problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'models' : models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_extended_2', 'wb') as file:\n",
    "    pickle.dump(model_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_extended', 'rb') as file:\n",
    "    models = pickle.load(file)['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_extended_2', 'rb') as file:\n",
    "    models = pickle.load(file)['models']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 50\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng()\n",
    "a_cn = 0.1 + rng.random() * (2.0 - 0.1)\n",
    "b_cn = rng.random() * 2 * np.pi\n",
    "x_train_cn = -5.0 + rng.random((n_train, 1)) * 10.0\n",
    "y_train_cn = a_cn * np.sin(x_train_cn + b_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train_cn, y_train_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_node = NN_1d_regression(id_node=1000, alpha=0.5, x_train=x_train_cn, y_train=y_train_cn, regularization=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_node.set_weights(control_node.w_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = control_node.model(torch.from_numpy(x_train_cn).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train_cn, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cn = -5.0 + rng.random((n_test, 1)) * 10.0\n",
    "y_test_cn = a_cn * np.sin(x_test_cn + b_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test_cn, y_test_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_cn = control_node.model(torch.from_numpy(x_test_cn).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test_cn, y_prediction_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_array = np.empty(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    control_node.alpha = 0.1 * i\n",
    "    control_node.set_weights(control_node.compute_local(models[i].w_opt_global))\n",
    "    y_prediction_cn = control_node.model(torch.from_numpy(x_test_cn).float()).detach().numpy()\n",
    "    mse_array[i] = np.mean((y_prediction_cn - y_test_cn) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_cn = control_node.model(torch.from_numpy(x_test_cn).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test_cn, y_prediction_cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOMAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tasks = 200\n",
    "number_of_points_per_task = 50\n",
    "dataset_name = 'artificial_sine_extended'\n",
    "\n",
    "alpha = 0.1\n",
    "n_workers = number_of_tasks\n",
    "exp = 'fomaml'\n",
    "max_it = 1\n",
    "\n",
    "alg = NN_1d_regression\n",
    "logreg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MasterNode(n_workers, alpha, alg, dataset_name, logreg, True, max_it, compute_smoothness_min=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4adec1aeef02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfomaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fed_mixture_code/workers.py\u001b[0m in \u001b[0;36mfomaml\u001b[0;34m(self, w, batch, n_iter)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfomaml_outer_loop_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfomaml_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fed_mixture_code/workers.py\u001b[0m in \u001b[0;36mfomaml_grad\u001b[0;34m(self, w, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[1;32m     88\u001b[0m         grad_vec = np.array([self.workers[i].fomaml_stochastic_grad(w, self.fomaml_number_of_inner_steps, self.fomaml_inner_loop_lr, batch) \n\u001b[0;32m---> 89\u001b[0;31m                              for i in self.workers])\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fed_mixture_code/workers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[1;32m     88\u001b[0m         grad_vec = np.array([self.workers[i].fomaml_stochastic_grad(w, self.fomaml_number_of_inner_steps, self.fomaml_inner_loop_lr, batch) \n\u001b[0;32m---> 89\u001b[0;31m                              for i in self.workers])\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fed_mixture_code/models.py\u001b[0m in \u001b[0;36mfomaml_stochastic_grad\u001b[0;34m(self, w, k, inner_loop_lr, batch)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfomaml_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_loop_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLogReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fed_mixture_code/models.py\u001b[0m in \u001b[0;36mfomaml_grad\u001b[0;34m(self, x, y, w, k, inner_loop_lr)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0minner_loop_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fed_mixture_code/models.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self, x, y, w)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_smoothness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fed_mixture_code/models.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(self, x, y, w)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fed_mixture_code/models.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mind\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0mind\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fomaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
