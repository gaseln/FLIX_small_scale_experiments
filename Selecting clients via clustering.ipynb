{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from multiprocessing.dummy import Pool\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "from tensorflow import strings\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DEVICE = 'cuda'\n",
    "INPUT_SIZE = 512\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.eval();\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Loaded 342477 client ids from SQL database.\n",
      "INFO:absl:Loaded 38758 client ids from SQL database.\n",
      "INFO:absl:Loaded 204088 client ids from SQL database.\n"
     ]
    }
   ],
   "source": [
    "stackoverflow_train, _, stackoverflow_test = tff.simulation.datasets.stackoverflow.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_caption(client_id):\n",
    "    return strings.join(inputs=[x['tokens'] for x in stackoverflow_train.create_tf_dataset_for_client(client_id)], separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_NUM = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pool = Pool(20)\n",
    "res = pool.map(client_caption, stackoverflow_train.client_ids[:CLIENT_NUM])\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_ids = stackoverflow_train.client_ids[:CLIENT_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf_strings_concatenated.data', 'wb') as file:\n",
    "    pickle.dump(res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/tf_strings_concatenated.data', 'rb') as file:\n",
    "    res = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 500 # restricted by tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256 # For batch_size=256 it takes about 3.5Gb of memory\n",
    "all_embeddings = {}\n",
    "\n",
    "captions = [x.numpy().decode(\"utf-8\").lower()[:INPUT_SIZE] for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for start_idx in tqdm(range(0, len(captions), batch_size)):\n",
    "    curr_captions = captions[start_idx : start_idx + batch_size]\n",
    "    tokens = tokenizer(curr_captions, return_tensors=\"pt\", padding=True)\n",
    "    tokens = tokens.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    curr_embeddings = outputs.pooler_output.cpu()\n",
    "    embeddings.extend(curr_embeddings)\n",
    "\n",
    "# Now, we would like to match image ids with the embeddings\n",
    "# so it will be easier for us to use it at test time\n",
    "result = {client_id: [] for client_id in sorted(list(set(client_ids)))}\n",
    "for client_id, emb in zip(client_ids, embeddings):\n",
    "    result[client_id].append(emb)\n",
    "\n",
    "# Now, let's save the resulted dict using pytorch save\n",
    "torch.save(result, 'datasets/embeddings_{}.pt'.format(CLIENT_NUM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.load('datasets/embeddings_5000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [y[0] for _, y in result.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two cluster of clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set of clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_ = [x.numpy() for x in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_embeddings = np.array(embeddings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from utils_federated.datasets import stackoverflow_tag_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Loaded 342477 client ids from SQL database.\n",
      "INFO:absl:Loaded 38758 client ids from SQL database.\n",
      "INFO:absl:Loaded 204088 client ids from SQL database.\n"
     ]
    }
   ],
   "source": [
    "train_fl, test_fl = stackoverflow_tag_prediction.get_federated_datasets(train_client_batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_client_ids = set(test_fl.client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "participating_client_numbers = []\n",
    "for i in range(CLIENT_NUM):\n",
    "    curr_client_id = client_ids[i]\n",
    "    if not curr_client_id in test_client_ids:\n",
    "        continue\n",
    "    \n",
    "    client_dataset = test_fl.create_tf_dataset_for_client(curr_client_id)\n",
    "    for element in client_dataset:\n",
    "        if element[0].shape[0] >= 50:\n",
    "            participating_client_numbers.append(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "participating_client_ids = [client_ids[i] for i in participating_client_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_embeddings = numpy_embeddings[participating_client_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=number_of_clusters, n_init=100, max_iter=1000, tol=1e-50, random_state=0, algorithm='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='full', max_iter=1000, n_clusters=10, n_init=100,\n",
       "       random_state=0, tol=1e-50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(X=numpy_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(kmeans.labels_, bins=number_of_clusters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = np.zeros(shape=(number_of_clusters,))\n",
    "for label in kmeans.labels_:\n",
    "    label_count[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40., 32., 41., 66., 27., 61.,  6., 33., 26., 63.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_40_labels = [i for i in range(number_of_clusters) if label_count[i] >= 40 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_client = {label: [] for label in range(number_of_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(participating_client_numbers)):\n",
    "    label_to_client[kmeans.labels_[idx]].append(participating_client_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in label_to_client:\n",
    "    if len(label_to_client[key]) != label_count[key]:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersecting_label_to_client = {label: [] for label in over_40_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_clients_list = sorted(list(set(stackoverflow_test.client_ids)))\n",
    "# for label in over_40_labels:\n",
    "#     for client in label_to_client[label]:\n",
    "#         if client in test_clients_list:\n",
    "#             intersecting_label_to_client[label].append(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in intersecting_label_to_client:\n",
    "#     print(label, len(intersecting_label_to_client[label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_labels = [label for label in intersecting_label_to_client if len(intersecting_label_to_client[label]) > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = over_40_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 5, 9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter, label in enumerate(selected_labels):\n",
    "    with open('datasets/clients_cluster_{}.data'.format(counter), 'wb') as file:\n",
    "        pickle.dump(label_to_client[label], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
