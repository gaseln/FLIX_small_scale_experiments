{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MulticlassLogReg\n",
    "import tensorflow_federated\n",
    "from utils_federated.datasets import stackoverflow_tag_prediction\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn.datasets import dump_svmlight_file, load_svmlight_file\n",
    "from prep_data import DATASET_PATH\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data & helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fl, test_fl = stackoverflow_tag_prediction.get_federated_datasets(train_client_batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_train_data_for_client(client_id, n=500, d_x = 10000, d_y =500):\n",
    "    one_client_data = train_fl.create_tf_dataset_for_client(client_id)\n",
    "    X = np.empty(shape = (n, d_x))\n",
    "    y = np.empty(shape = (n, d_y))\n",
    "    for element in one_client_data:\n",
    "        X = element[0].numpy()\n",
    "        y = element[1].numpy()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_num = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_keyword = 'SO_LR_one_cluster_{}'.format(client_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 90\n",
    "n_test_samples = 300\n",
    "n_val_samples = 110\n",
    "\n",
    "assert n_train_samples + n_test_samples + n_val_samples == 500\n",
    "\n",
    "X = np.empty(shape = (n_train_samples * client_num, 10000))\n",
    "y = np.empty(shape = (n_train_samples * client_num, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "participating_clients = []\n",
    "rng = default_rng()\n",
    "cluster = 3\n",
    "counter = 0\n",
    "max_labels_used = 0\n",
    "\n",
    "with open('datasets/clients_cluster_{}.data'.format(cluster), 'rb') as file:\n",
    "    selected_clients = pickle.load(file)\n",
    "    \n",
    "X_ = None\n",
    "y_ = None\n",
    "train_indices = None\n",
    "val_indices = None\n",
    "test_indices = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# clients added 50, # clients required 50\n"
     ]
    }
   ],
   "source": [
    "for client_id in selected_clients:\n",
    "    if counter == client_num:\n",
    "        break    \n",
    "    X_, y_ = retrieve_train_data_for_client(client_id)\n",
    "    if X_.shape[0] != 500:\n",
    "        continue\n",
    "    max_labels_used = max(max_labels_used, np.array([1 for x in y_.sum(axis=0) if x > 0]).sum())\n",
    "    participating_clients.append(client_id)\n",
    "\n",
    "    y_sum = y_.sum(axis=0)\n",
    "    assert y_sum.shape[0] == 500\n",
    "\n",
    "    all_indices = set(list(range(500)))\n",
    "    participating_labels = [i for i in range(500) if y_sum[i] > 0]\n",
    "    unique_datapts = set()\n",
    "    for label in participating_labels:\n",
    "        for ind in range(500):\n",
    "            if y_[ind, label] == 1.:\n",
    "                unique_datapts.add(ind)\n",
    "                break\n",
    "        \n",
    "    rest_inds = list(all_indices - unique_datapts)\n",
    "    rng.shuffle(rest_inds)\n",
    "    all_indices = list(unique_datapts) + rest_inds\n",
    "    \n",
    "    train_indices = all_indices[:n_train_samples]\n",
    "    val_indices = all_indices[n_train_samples : n_train_samples + n_val_samples]\n",
    "    test_indices = all_indices[n_train_samples + n_val_samples:]\n",
    "        \n",
    "    X[counter * n_train_samples : (counter + 1) * n_train_samples, :] = X_[train_indices, :]\n",
    "    y[counter * n_train_samples : (counter + 1) * n_train_samples, :] = y_[train_indices, :]\n",
    "    with open('datasets/{}_val{}_X.npy'.format(exp_keyword, client_id), 'wb') as file_1:\n",
    "        np.save(file_1, X_[val_indices, :])\n",
    "    with open('datasets/{}_val{}_y.npy'.format(exp_keyword, client_id), 'wb') as file_2:\n",
    "        np.save(file_2, y_[val_indices, :])                        \n",
    "    with open('datasets/{}_test{}_X.npy'.format(exp_keyword, client_id), 'wb') as file_3:\n",
    "        np.save(file_3, X_[test_indices, :])\n",
    "    with open('datasets/{}_test{}_y.npy'.format(exp_keyword, client_id), 'wb') as file_4:\n",
    "        np.save(file_4, y_[test_indices, :])\n",
    "    counter += 1\n",
    "print('# clients added {}, # clients required {}'.format(counter, client_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/{}.npz'.format(exp_keyword), 'wb') as file:\n",
    "    np.savez(file, X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_PATH + 'list_clients_{}.data'.format(exp_keyword), 'wb') as file:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(participating_clients, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two client datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_clients = sorted(list(set(train_fl.client_ids).intersection(set(test_fl.client_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "participating_clients = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    client_id = common_clients[i]\n",
    "    train_d = train_fl.create_tf_dataset_for_client(client_id)\n",
    "    test_d = test_fl.create_tf_dataset_for_client(client_id)\n",
    "    tr_flag = False\n",
    "    te_flag = False\n",
    "    for d in train_d:\n",
    "        if d[0].shape[0] == 500:\n",
    "            tr_flag = True\n",
    "            break\n",
    "    for d in test_d:\n",
    "        if d[0].shape[0] > 400:\n",
    "            te_flag = True\n",
    "            break\n",
    "    if tr_flag and te_flag:\n",
    "        participating_clients.append(client_id)\n",
    "    if len(participating_clients) > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00000267',\n",
       " '00000459',\n",
       " '00000476',\n",
       " '00001288',\n",
       " '00001337',\n",
       " '00001338',\n",
       " '00001831',\n",
       " '00001968',\n",
       " '00002988',\n",
       " '00003043',\n",
       " '00003501']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participating_clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, y_ = retrieve_train_data_for_client(participating_clients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist = y_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 35, 48, 19, 157, 193, 83, 253, 26, 303]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(range(len(label_dist)), key = lambda k: label_dist[k], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, y_ = retrieve_train_data_for_client(participating_clients[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist = y_.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[230, 2, 13, 12, 63, 0, 81, 434, 472, 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(range(len(label_dist)), key = lambda k: label_dist[k], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty(shape=(1000, 10000))\n",
    "y = np.empty(shape=(1000, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    curr_client_id = participating_clients[i]\n",
    "    X_, y_ = retrieve_train_data_for_client(curr_client_id)\n",
    "    X[i * 500 : (i + 1) * 500, :] = X_\n",
    "    y[i * 500 : (i + 1) * 500, :] = y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/SO_LR_two_workers_100.npz', 'wb') as file:\n",
    "    np.savez(file, X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_PATH + 'list_clients_SO_LR_two_workers_100.data', 'wb') as file:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(participating_clients[:2], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_SIZE = 300\n",
    "VAL_DATA_SIZE = 100\n",
    "for client_id in participating_clients:\n",
    "    test_data = test_fl.create_tf_dataset_for_client(client_id)\n",
    "    X_test = np.empty(shape = (TEST_DATA_SIZE, 10000))\n",
    "    y_test = np.empty(shape = (TEST_DATA_SIZE, 500))\n",
    "    X_val = np.empty(shape = (VAL_DATA_SIZE, 10000))\n",
    "    y_val = np.empty(shape = (VAL_DATA_SIZE, 500))\n",
    "    \n",
    "    write_flag = False\n",
    "    for element in test_data:\n",
    "        if element[0].shape[0] < TEST_DATA_SIZE + VAL_DATA_SIZE:\n",
    "            continue\n",
    "        \n",
    "        X_test = element[0].numpy()[:TEST_DATA_SIZE, :]\n",
    "        y_test = element[1].numpy()[:TEST_DATA_SIZE, :]\n",
    "        \n",
    "        X_val = element[0].numpy()[TEST_DATA_SIZE : TEST_DATA_SIZE + VAL_DATA_SIZE, :]\n",
    "        y_val = element[1].numpy()[TEST_DATA_SIZE : TEST_DATA_SIZE + VAL_DATA_SIZE, :]\n",
    "        \n",
    "        write_flag = True\n",
    "        \n",
    "    if not write_flag:\n",
    "        raise RuntimeError\n",
    "        \n",
    "    with open('datasets/SO_LR_two_workers_100_test{}_X.npy'.format(client_id), 'wb') as file_1:\n",
    "        np.save(file_1, X_test)\n",
    "    with open('datasets/SO_LR_two_workers_100_test{}_y.npy'.format(client_id), 'wb') as file_2:\n",
    "        np.save(file_2, y_test)  \n",
    "    with open('datasets/SO_LR_two_workers_100_val{}_X.npy'.format(client_id), 'wb') as file_3:\n",
    "        np.save(file_3, X_val)\n",
    "    with open('datasets/SO_LR_two_workers_100_val{}_y.npy'.format(client_id), 'wb') as file_4:\n",
    "        np.save(file_4, y_val)          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
