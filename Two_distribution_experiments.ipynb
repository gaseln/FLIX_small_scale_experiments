{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workers import MasterNode\n",
    "from models import LinReg, LogReg, LogRegNoncvx, NN_1d_regression\n",
    "from utils import read_run, get_alg, create_plot_dir, PLOT_PATH\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from prep_data import number_of_features\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from numpy.random import default_rng\n",
    "from numpy import linalg as la\n",
    "from prep_data import DATASET_PATH\n",
    "import copy\n",
    "import pickle\n",
    "import sys\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fast')\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "# mpl.rcParams['mathtext.fontset'] = 'dejavusans'\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['lines.linewidth'] = 2.0\n",
    "mpl.rcParams['legend.fontsize'] = 'large'\n",
    "mpl.rcParams['axes.titlesize'] = 'xx-large'\n",
    "mpl.rcParams['xtick.labelsize'] = 'x-large'\n",
    "mpl.rcParams['ytick.labelsize'] = 'x-large'\n",
    "mpl.rcParams['axes.labelsize'] = 'xx-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = ['x', '.', '+', '1', 'p','*', 'D' , '.',  's']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(number_of_tasks,\n",
    "                    n1, \n",
    "                    number_of_points_per_task, \n",
    "                    number_of_points_per_task_for_validation, \n",
    "                    dataset_name,\n",
    "                    validation_dataset_name,\n",
    "                    keyword\n",
    "                   ):    \n",
    "    X_sine = np.empty(shape=(number_of_tasks * number_of_points_per_task, 1))\n",
    "    y_sine = np.empty(shape=(number_of_tasks * number_of_points_per_task))\n",
    "    X_sine_val = np.empty(shape=(number_of_tasks * number_of_points_per_task_for_validation, 1))\n",
    "    y_sine_val = np.empty(shape=(number_of_tasks * number_of_points_per_task_for_validation))\n",
    "    \n",
    "    n2 = number_of_tasks - n1\n",
    "    \n",
    "    rng = default_rng()\n",
    "    a_array = []\n",
    "    b_array = []\n",
    "    a_array_old = np.load(DATASET_PATH + 'a_two_distribution_validated.npy')\n",
    "    b_array_old = np.load(DATASET_PATH + 'b_two_distribution_validated.npy')\n",
    "    dist_sizes = [n1, n2]\n",
    "    \n",
    "    for j in range(2):\n",
    "        a = a_array_old[100 * j]\n",
    "        b = b_array_old[100 * j]\n",
    "        for i in range(dist_sizes[j]):\n",
    "            a_array.append(a)\n",
    "            b_array.append(b)\n",
    "\n",
    "            x_train = -5.0 + rng.random((number_of_points_per_task, 1)) * 10.0\n",
    "            x_val = -5.0 + rng.random((number_of_points_per_task_for_validation, 1)) * 10.0\n",
    "            y_train = a * np.sin(x_train + b)\n",
    "            y_val = a * np.sin(x_val + b)\n",
    "\n",
    "            ind = i + dist_sizes[0] * j\n",
    "            X_sine[number_of_points_per_task * ind : number_of_points_per_task * (ind + 1)] = x_train.copy()\n",
    "            X_sine_val[number_of_points_per_task_for_validation * ind : number_of_points_per_task_for_validation * (ind + 1)] = x_val.copy()    \n",
    "            y_sine[number_of_points_per_task * ind : number_of_points_per_task * (ind + 1)] = y_train.squeeze(1).copy()\n",
    "            y_sine_val[number_of_points_per_task_for_validation * ind : number_of_points_per_task_for_validation * (ind + 1)] = y_val.squeeze(1).copy()    \n",
    "    a_array = np.array(a_array)\n",
    "    b_array = np.array(b_array)\n",
    "    dump_svmlight_file(X_sine, y_sine, DATASET_PATH + dataset_name)\n",
    "    dump_svmlight_file(X_sine_val, y_sine_val, DATASET_PATH + validation_dataset_name)\n",
    "    np.save(DATASET_PATH + 'a' + keyword + '.npy', a_array)\n",
    "    np.save(DATASET_PATH + 'b' + keyword + '.npy', b_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(models, filename='a_constant_1000'):\n",
    "    models_dict = {'models' : models}\n",
    "    with open(MODELS_PATH + filename, 'wb') as file:\n",
    "        pickle.dump(models_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_model(number_of_tasks, \n",
    "                 dataset_name, \n",
    "                 validation_dataset_name,\n",
    "                 keyword,\n",
    "                 model\n",
    "                ):\n",
    "    models = []\n",
    "    for i in range(11):\n",
    "        models.append(copy.deepcopy(model))\n",
    "    alphas = np.linspace(0, 1.0, 11)\n",
    "    for i in range(1,11):\n",
    "        models[i].change_alpha(alphas[i])\n",
    "    models[0].change_alpha(0.0)\n",
    "    models[0].w_opt_global = np.zeros(models[0].d)\n",
    "    saves_name = 'two_distribution' + keyword\n",
    "    for i in range(1, 11):\n",
    "        model = models[i]\n",
    "        min_L = 0.1\n",
    "        max_L = 0.1\n",
    "        max_it = 10000\n",
    "        tol = 1e-2\n",
    "        max_L_constant = 2 ** 40\n",
    "        w = copy.deepcopy(models[i-1].w_opt_global)\n",
    "        grad_norm = None\n",
    "        min_f_value = float('Inf')\n",
    "\n",
    "        try:\n",
    "            for it in range(max_it):\n",
    "                grad = model.grad(w)\n",
    "                L = min_L\n",
    "                curr_fun_value = model.fun_value(w)\n",
    "\n",
    "                while True:\n",
    "                    if L > max_L_constant: # if L becomes too large, jump to another random point w\n",
    "                        w = np.random.randn(model.d)\n",
    "                        grad = model.grad(w)\n",
    "                        L = min_L\n",
    "                        curr_fun_value = model.fun_value(w)\n",
    "\n",
    "                    print('Current L = {:f}'.format(L), end='\\r')\n",
    "\n",
    "                    f_value_ = model.fun_value(w - grad / L)\n",
    "                    if curr_fun_value - f_value_ > 0:\n",
    "                        break\n",
    "                    L *= 2.0\n",
    "\n",
    "                w -= grad / L\n",
    "                grad_norm = la.norm(grad)\n",
    "\n",
    "                if f_value_ is None:\n",
    "                    raise Exception('None is detected') \n",
    "\n",
    "                if f_value_ < min_f_value:\n",
    "                    min_f_value = f_value_\n",
    "                    model.w_opt_global = copy.deepcopy(w)\n",
    "\n",
    "                if max_L < L:\n",
    "                    max_L = L\n",
    "                print('                               {:5d}/{:5d} Iterations: fun_value {:f} grad_norm {:f}'.format(it+1, max_it, f_value_, grad_norm), end='\\r')                \n",
    "                if grad_norm < tol and f_value_ < tol ** 2:\n",
    "                    save(models, saves_name)\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            print('')\n",
    "            print(min_f_value)\n",
    "        else:\n",
    "            save(models, saves_name)\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_distribution(number_of_tasks, first_dataset_size, validation=True):\n",
    "    n1 = first_dataset_size\n",
    "    n2 = number_of_tasks - n1\n",
    "    keyword = str(n1) + 'vs' + str(n2)\n",
    "    if validation:\n",
    "        keyword += 'with_validation_stop'\n",
    "    dataset_name = 'artificial_sine_two_distribution' + keyword\n",
    "    validation_dataset_name = 'artificial_sine_two_distribution_validation' + keyword\n",
    "    \n",
    "    file_out = open(keyword + '.txt', 'w')\n",
    "    sys.stdout = file_out\n",
    "       \n",
    "    model = MasterNode(n_workers=number_of_tasks, \n",
    "                   alpha=0.05, \n",
    "                   worker=NN_1d_regression, \n",
    "                   dataset_name=dataset_name, \n",
    "                   logreg=False, \n",
    "                   ordered=True, \n",
    "                   max_it=100, \n",
    "                   tolerance=1e-2, \n",
    "                   validation=validation, \n",
    "                   validation_dataset_name=validation_dataset_name)\n",
    "  \n",
    "    global_model(number_of_tasks, dataset_name, validation_dataset_name,keyword, model)\n",
    "    file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating datasets\n",
    "# for i in np.arange(10, 110, 10):\n",
    "#         number_of_tasks = 200\n",
    "#     n1 = i\n",
    "#     n2 = number_of_tasks - n1\n",
    "#     number_of_points_per_task = 50\n",
    "#     number_of_points_per_task_for_validation = 20\n",
    "#     keyword = str(n1) + 'vs' + str(n2)\n",
    "#     dataset_name = 'artificial_sine_two_distribution' + keyword\n",
    "#     validation_dataset_name = 'artificial_sine_two_distribution_validation' + keyword\n",
    "    \n",
    "#     create_datasets(number_of_tasks,\n",
    "#                 n1, \n",
    "#                 number_of_points_per_task, \n",
    "#                 number_of_points_per_task_for_validation, \n",
    "#                 dataset_name,\n",
    "#                 validation_dataset_name,\n",
    "#                 keyword\n",
    "#                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.arange(10, 110, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation stop critetion experiments\n",
    "processes = []\n",
    "ind = 0\n",
    "for first_dataset_size in sizes:\n",
    "    processes.append(Process(target=two_distribution, args=(200, first_dataset_size, True)))\n",
    "    processes[ind].start()\n",
    "    ind += 1\n",
    "for ind in range(sizes.size):\n",
    "    processes[ind].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcvalue stop criterion\n",
    "processes = []\n",
    "ind = 0\n",
    "for first_dataset_size in sizes:\n",
    "    processes.append(Process(target=two_distribution, args=(200, first_dataset_size, False)))\n",
    "    processes[ind].start()\n",
    "    ind += 1\n",
    "for ind in range(sizes.size):\n",
    "    processes[ind].join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = True\n",
    "if validation:\n",
    "    print('Drawing graphs for mixed models with validation stop criterion')    \n",
    "else:\n",
    "    print('Drawing graphs for mixed models with functional value stop criterion')\n",
    "    \n",
    "for first_dataset_size in sizes:\n",
    "    keyword = str(first_dataset_size) + 'vs' + str(200 - first_dataset_size)\n",
    "    \n",
    "    a_array = np.load(DATASET_PATH + 'a' + keyword + '.npy')\n",
    "    b_array = np.load(DATASET_PATH + 'b' + keyword + '.npy')\n",
    "    if validation:\n",
    "        keyword += 'with_validation_stop'\n",
    "    model_name = 'two_distribution' + keyword\n",
    "    with open(MODELS_PATH + model_name , 'rb') as file:\n",
    "        models = pickle.load(file)['models']\n",
    "        \n",
    "    mse_table = np.empty(shape=(11, models[0].n_workers))\n",
    "    \n",
    "    n_test = 2000\n",
    "    rng = default_rng()\n",
    "    for i in range(models[0].n_workers):\n",
    "        x_test = -5.0 + rng.random((n_test, 1)) * 10\n",
    "        y_test = a_array[i] * np.sin(x_test + b_array[i])\n",
    "        for j in range(11):\n",
    "            worker = models[j].workers[i]\n",
    "            worker.set_weights(worker.compute_local(models[j].w_opt_global))\n",
    "            y_pred = worker.model(torch.from_numpy(x_test).float()).detach().numpy()\n",
    "            mse = np.mean((y_pred - y_test) ** 2).item()\n",
    "            mse_table[j][i] = copy.copy(mse)\n",
    "            \n",
    "    mse_alpha = np.mean(mse_table, axis=1)\n",
    "    alphas = np.linspace(0, 1.0, 11)\n",
    "    argmin = np.argmin(mse_alpha)\n",
    "    alpha_min = alphas[argmin]\n",
    "    plt.figure()\n",
    "    plt.plot(alphas, mse_alpha, marker='o')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('Average MSE over clients')\n",
    "    plt.xticks(alphas)\n",
    "    plt.axvline(x=alpha_min, ymin = 0, ymax=1, ls='--')\n",
    "    plt.title(keyword)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_PATH + '/' + keyword + '.pdf')\n",
    "    plt.figure()\n",
    "    mse_best_log = np.log(mse_table[argmin])\n",
    "    plt.hist(mse_best_log, bins=50)\n",
    "    plt.xlabel('Log MSE for the best alpha')\n",
    "    plt.title(keyword)\n",
    "    plt.savefig(PLOT_PATH + '/hist' + keyword + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
